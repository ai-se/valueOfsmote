
\documentclass[conference]{IEEEtran}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{blindtext, graphicx}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{float}
\usepackage{balance}
\usepackage{listings}
\renewcommand{\thesubsection}{\arabic{subsection}}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{comment}
\usepackage{framed}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{bigstrut}
\usepackage{color}
\usepackage{graphics}
\usepackage{eqparbox}
\usepackage{graphics}
\usepackage{colortbl}
\usepackage{paralist}
%\usepackage{times}
\usepackage{mathptmx} 
\usepackage{courier}
\usepackage{picture}
\usepackage[shortlabels]{enumitem}
\usepackage{url}

\newcommand{\bi}{\begin{itemize}[leftmargin=0.4cm]}
	\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
	\newcommand{\ee}{\end{enumerate}}

\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{hhline}
\ifCLASSINFOpdf
 
\else
  
\fi



% correct bad hyphenation here
\hyphenation{}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Handling Imbalance data; lessons learnt from SMOTE}


\author{\IEEEauthorblockN{Amritanshu Agrawal}
\IEEEauthorblockA{Department of Computer Science\\
North Carolina State University\\
Raleigh, NC, USA\\
Email: aagrawa8@ncsu.edu}
\and
\IEEEauthorblockN{Tim Menzies}
\IEEEauthorblockA{Department of Computer Science\\
North Carolina State University\\
Raleigh, NC, USA\\
Email: tim@menzies.us}}
\maketitle

\maketitle


\begin{abstract}
Software Engineering (SE) is complex, hence, data associated with SE is inherently complex and noisy. Prior in Software analytics, it is a common practice to preprocess the data. For example, when handling imbalance data, standard practice is to apply SMOTE (Synthetic Minority Oversampling Technique) on imbalanced defect prediction data. We show that ``off-the-shelf'' SMOTE should be deprecated in favor of tuning SMOTE with Search Based Software Engineering (SBSE). 

As evidence for this we analyzed 14 datasets with varying imbalance to show that (a) ``off-the-shelf'' SMOTE often decreasing performance, (b) SMOTE+tuning improves performance results by 10-15\%, and (c) the improvements achieved by SMOTE$+$tuning differs according to which evaluation metric is applied
\end{abstract}

\begin{IEEEkeywords}
defect prediction, classification, SMOTE, search based software engineering, imbalance data
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle


Software defect prediction has been an important research topic in the software engineering field for more than 30 years. It has generated widespread interest for a
considerable period of time. The driving scenario is resource
allocation: Time and manpower being finite resources, it
makes sense to assign personnel and/or resources to areas of
a software system with a higher probable quantity of defects. Current defect prediction work focuses on (i) estimating the number of defects remaining in software systems, (ii) discovering defect associations, and (iii) classifying the defect-proneness of software components, typically into two classes defect-prone and not defect-prone. 

There has been vast amount of studies done to find the best defect prediction performing model which is related to third type of problem. But literature suggests, that no single prediction technique dominates and making sense of the many prediction results is hampered by the use of different data sets, data pre-processing, validation schemes and performance
statistics. We highly agree to this given so many variations available in the data and there are so many classification techniques available like Statistical, Clustering, Rule-Based, Neural Networks, Nearest Neighbour, Support Vector Machines, Decision trees, ensemble methods, to name a few.

Result by Tantithamthavorn et al~\cite{tantithamthavorn2016automated} also suggested that every dataset comes with different attributes. And also classification techniques often have configurable parameters
that control characteristics of these classifiers that they produce. Now time has come to even think about hyperparameter optimization of these techniques and come up with an automated process~\cite{agrawal2016wrong, fu2016tuning} to tune these parameters for every dataset.

This paper deals with the third type of problem for code metrics which is classifying the defect-proneness of software components, typically into two classes, defective and not defective. Ghotra et al~\cite{ghotra2015revisiting} did a comparative study on various learners for defect prediction. They found out that mainly 6 learners have been performing well, namely, Naive Bayes, Logistic regression, Support Vector Machines, Nearest Neighbor, decision tree and Random forest.

The other problem which we face is imbalanced classes of these datasets. Usually most software systems usually have less than 20\% defective classes. And we want each software quality team to allocate their limited resources to the most defect-prone
modules rather than wasting on non-defective modules. This gives us the motivation to find different ways of balancing the minority class combined with a good prediction model to solve the problem of defect prediction.

For the reproduction package we generalised our datasets to be comprised of CK metrics~\cite{chidamber1994metrics}. The CK metrics aim at measuring whether a piece of code follows OO principles. It contains a check of these OO design attributes: Weighted Methods for Class (WMC), Depth of Inheritance Tree (DIT), Number of Children (NOC), Response for Class (RFC), Lack of Cohesion of Methods (LCOM), and Coupling Between Objects (CBO)

We created a python package generalised to run any CK metrics based dataset and compare results against 6 learners. Since the classes are imbalanced we used SMOTE~\cite{chawla2002smote} (only on Training Data) which is a synthetic minority over-sampling technique.

The remainder of the paper is organized as follows. Section \ref{review} gives a brief related work on defect prediction. Section \ref{motivation} talks about why there is a need to balance the data. Since we found astonishing results with smote, section \ref{smote} talks about SMOTE in defect prediction. Experimental setup is provided in section \ref{experiment}. Results are discussed in Section \ref{results}. Threats to validity section is discussed in section \ref{validity}. Final conclusion is being discussed in section \ref{conclusion}. And section \ref{future} talks about our future work.

\section{Related Work}
\label{review}

There are works on estimating the number of defects remaining in software systems which employs statistical approaches, capture-recapture (CR) models, and detection profile methods (DPM)~\cite{song2011general}. The other problem of revealing software defect associations is done using association rule mining
algorithms~\cite{song2006software}. A variety of approaches have been proposed to tackle the problem of classifying the defect-proneness of software components. It is heavily relied on diverse information, such as code metrics~\cite{d2010extensive,menzies2007data, nagappan2006mining,shepperd2014researcher} (lines of code, complexity), process metrics~\cite{hassan2009predicting} (number of changes, recent activity) or previous defects~\cite{kim2007predicting}.

Bird et al~\cite{bird2009putting} indicate that it is possible to predict which components are likely locations of
defect occurrence using a component's development history,
and dependency structure. Two key properties of software components
in large systems are dependency relationships (which components
depend on or are dependent on by others), and development
history (who made changes to the components and
how many times). Thus we can link software components
to other components a) in terms of their dependencies, and
also b) in terms of the developers that they have in common. Prediction models based on the topological properties
of components within them have proven to be quite
accurate~\cite{zimmermann2008predicting}.

By keeping change logs of the most recently or frequently changed files are the most probable source of future defects~\cite{d2010extensive, hall2012systematic, catal2009systematic}. They compared various code metrics like CK  metrics  suite,  McCabes  cyclomatic  complexity, Briands coupling metrics, code metrics, dependencies between  binaries, and cohesion  measurement based  on LSI. But what they found is CK metrics combined with OO (object-oriented) metrics perform better than all other metrics.  There is added advantage that comes with CK+OO metrics. They are  lightweight  to  compute,  have good  explanative  and  predictive  power  and  do  not require historical information. And OO metrics (49\%) were used nearly twice as often compared to traditional source code metrics (27\%) or process metrics (24\%)~\cite{radjenovic2013software}. Chidamber and Kemerer's (CK) objected-oriented metrics were most frequently used.

And lastly we found a paper from Ghotra et al~\cite{ghotra2015revisiting} on "Revisiting the impact of classification techniques on the performance of defect prediction models". To  compare  the  performance  of  defect prediction  models,  they  used  the  Area  Under  the receiver operating characteristic Curve (AUC), which plots  the  false  positive  rate  against  the  true  positive rate. They ran the Scott-Knott test to group classification techniques into statistically distinct ranks. After running these evaluation criteria, they concluded that Naive Bayes, Logistic regression, Support Vector Machines, Nearest Neighbor, decision tree and Random forest performs the best depending on various datasets.

\section{Motivation for balancing the class}
\label{motivation}

Class imbalance learning refers to learning from data sets that exhibit significant imbalance among or within classes. The common understanding about class imbalance in the literature is concerned with the situation in which some classes of data are
highly under-represented compared to other classes~\cite{he2009learning}. By convention,
the under-represented class is called the minority class,
and correspondingly the class having the larger size is called the
majority class.

Misclassifying an example from the minority class is usually more costly. For software defect prediction, due to the nature of the problem, the defect case is much less likely to happen than the non-defect case.
The defective class is thus the minority. The recognition of this class is more important, because the failure of finding a defect could degrade software quality greatly. The learning objective can be generally described
as ``obtaining a classifier that will provide high accuracy for the minority class without severely compromising the accuracy of the majority class''.

Numerous methods have been proposed to tackle class
imbalance problems at data and algorithm levels. Data-level include a variety of resampling techniques, manipulating training data to rectify the skewed class distributions, such as random oversampling, random undersampling, and SMOTE~\cite{estabrooks2004multiple}. Algorithm-level methods address class imbalance by
modifying their training mechanism directly with the 
goal of better accuracy on the minority class, including cost-sensitive learning algorithms ~\cite{he2009learning}.
Algorithm-level methods require specific treatments for different
kinds of learning algorithms, which hinders their use
in many applications, because we do not know in advance
which algorithm would be the best choice in most cases. In addition to the aforementioned data-level and algorithm-level solutions, ensemble learning has become another major category of approaches to handle imbalanced data by combining multiple classifiers, such as SMOTEBoost~\cite{chawla2003smoteboost}, and
AdaBoost.NC~\cite{wang2010negative}. But none of these methods have investigated thoroughly on class imbalance problem by comparing so many learners.

Hall et al~\cite{hall2012systematic} found that models based on C4.5 seem to underperform if they have imbalanced data. They suggested data should never be imbalanced. Naive Bayes and Logistic regression, in particular, seem to be the techniques used in models that are performing relatively well.

Wang et al~\cite{wang2013using} studied various undersampling and oversampling technique and compared the results with Naive Bayes and random forest. And found out that techniques like AdaBoost.NC had a better performance than the rest while others are planning to use SMOTE~\cite{gray2009using}. Yan et al~\cite{yan2010software} performed fuzzy logic and rules to overcome the imbalance problem only to work with Support Vector Machines. 

Pelayo et al~\cite{pelayo2007applying} studied the effects of percentage of oversampling and undersampling done. They found out that different percentage of each helps improve the accuracies of decision tree learner for defect prediction using CK metrics. Menzies et al~\cite{menzies2008implications} undersampled the non-defect class to balance training
data, and checked how little information was required to learn a defect predictor. They found that throwing away data does not degrade the performance of Naive Bayes and C4.5 decision trees, and instead improves the performance of C4.5. Some other papers also showed the usefulness of resampling based on different learners~\cite{pelayo2007applying, pelayo2012evaluating, riquelme2008finding}.

\section{SMOTE in Defect Prediction}
\label{smote}
There have been various oversampling and undersampling techniques available. And SMOTE~\cite{chawla2002smote} has become increasingly popular in recent times. SMOTE works by creating a new minority-class sample at a random point on the line connecting a minority class example with its nearest neighbor (of the same class) in feature space. It first selects instances from the minority class and finds k nearest neighbors for each instance, where k is a given number. It then creates new instances using the selected instances and their neighbors. 

Pears et al~\cite{pears2014synthetic} used SMOTE to study software build outcomes. They didn't generalize the results for every datasets. They observed
that classification accuracy steadily improves after creating synthetically approximately 900 instances of builds that have been fed to the classifier. Tan et al~\cite{tan2015online} investigated on online defect prediction for imbalance data. They studied resampling techniques and found improvement in precision by 12.2–89.5\% or 6.4–34.8 percentage points. Removing testing-related changes can improve F1 by 62.2–3411.1\% or 19.4–61.4 percentage points. while achieving a comparable precision.

Pelayo~\cite{pelayo2007applying} found out that by using SMOTE, there wasn't any improvement. But using other resampling strategies like trial-and-error, and they arrived at the highest geometric mean accuracies. Kamei et al~\cite{kamei2007effects} evaluated the effects of SMOTE applied to only four fault-proneness models
(linear discriminant analysis, logistic regression
analysis, neural network and classification tree) by
using two module sets of industry legacy software. They reported SMOTE improved the prediction performance of the linear and logistic models, while neural network and classification tree models did not
benefit from it. In~\cite{van2007experimental} it is identified that classifier performance is improved with SMOTE, but individual learners respond differently on sampling. We will be studying many more models and will show that SMOTE does help in all prediction models.

\section{Experimental Setup}
\label{experiment}
\section{Conclusion}
\label{conclusion}

We were able to reproduce the baseline paper "Revisiting the impact of classification techniques on the performance of defect prediction models" with the same conclusions what Ghotra et al found which are results without smote. But some other conclusions are also found based on our study. Naive Bayes should be performed if you have imbalance dataset. Smote should be applied to balance any minority class whether its a defective class or non-defective. You will get better results. Even To control the high variance and most data sets have minority defective class, we highly recommend to use smoting. Comparing the run times and performance, we suggest to use Random Forest if the data sets are not big as it has larger runtime overhead.

\section{Future Work}
\label{future}
In future, we can improve this pip package once other users start reporting issues. Additional features can be added such as:
\begin{itemize}
 \item We can think of implementing cross project defect prediction.
 \item We can have binary classification, as well as to predict quantities of defects which is regression based model.
 \item The algorithm for smoting is hard-coded to "ball tree", this can be parametrized.
 \item The Split criteria and K value in K Nearest Neighbours are hard-coded, these can be parametrized.
 \item The learners currently does not support any tuning, which can implemented.
 \item Pretty visualizations can be added.
 \item Preprocessing to remove few instances and other steps just like in~\cite{gray2009using}.
\end{itemize}

\balance

\bibliographystyle{abbrv}
\medskip
\bibliography{main}

\end{document}